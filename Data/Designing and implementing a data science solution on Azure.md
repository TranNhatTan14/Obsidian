---
tags:
  - Course
---

# Design a machine learning solution

## Design a data ingestion strategy for machine learning projects

Data is the foundation of machine learning. Both data quantity and data quality will affect the model’s accuracy.

Imagine you're a data scientist and have been asked to train a machine learning model.

You aim to go through the following six steps to plan, train, deploy, and monitor the model:

![[Pasted image 20240819094430.png]]

1. **Define the problem**: Decide on what the model should predict and when it's successful.
2. **Get the data**: Find data sources and get access.
3. **Prepare the data**: Explore the data. Clean and transform the data based on the model's requirements.
4. **Train the model**: Choose an algorithm and hyperparameter values based on trial and error.
5. **Integrate the model**: Deploy the model to an endpoint to generate predictions.
6. **Monitor the model**: Track the model's performance.

In general, it’s a best practice to extract data from its source before analyzing it. Whether you’re using the data for data engineering, data analysis, or data science, you’ll want to **extract** the data from its source, **transform** it, and **load** it into a serving layer. Such a process is also referred to as **Extract, Transform, and Load** (**ETL**) or **Extract, Load, and Transform** (**ELT**). The serving layer makes your data available for the service you’ll use for further data processing like training machine learning models.

Before being able to design the ETL or ELT process, you’ll need to **identify your data source and data format**

## Identify the data source

When you start with a new machine learning project, first identify where the data you want to use is stored.

The necessary data for your machine learning model may already be stored in a database or be generated by an application. For example, the data may be stored in a Customer Relationship Management (CRM) system, in a transactional database like an SQL database, or be generated by an Internet of Things (IoT) device.

In other words, your organization may already have business processes in place, which generate and store the data. If you don’t have access to the data you need, there are alternative methods. You can collect new data by implementing a new process, acquire new data by using publicly available datasets, or buy curated datasets.

## Identify the data format

Based on the source of your data, understand the format of the data and determine the format required for your machine learning workloads.

Commonly, we refer to three different formats:

- **Tabular** or **structured** data: All data has the same fields or properties, which are defined in a schema. Tabular data is often represented in one or more tables where columns represent features and rows represent data points. 
- **Semi-structured** data: Not all data has the same fields or properties. Instead, each data point is represented by a collection of _key-value pairs_. The keys represent the features, and the values represent the properties for the individual data point. For example, real-time applications like Internet of Things (IoT) devices generate a JSON object:
- **Unstructured** data: Files that don't adhere to any rules when it comes to structure. For example, documents, images, audio, and video files are considered unstructured data. Storing them as unstructured files ensures you don’t have to define any schema or structure, but also means you can't ==query== the data in the database. You'll need to specify how to read such a file when consuming the data.

## Identify the desired data format

When extracting the data from a source, you may want to transform the data to change the data format and make it more suitable for model training.

Once you’ve identified the data source, the original data format, and the desired data format, you can think about how you want to serve the data. Then, you can ==design a data ingestion pipeline== to automatically extract and transform the data you need.

## Choose how to serve data to machine learning workflows

To access data when training machine learning models, you'll want to serve the data by storing it in a cloud data service. By storing data separately from your compute, you’ll ==minimize costs and be more flexible==.

### Separate compute from storage

One of the benefits of the cloud is the ability to scale compute up or down according to your demands. In addition, you can shut down compute when you don’t need it and restart it when you want to use it again.

Especially when training machine learning models, you’ll have periods of time during which you'll need a lot of compute power, and times when you don’t. When shutting down the compute you use for training machine learning models, you want to ensure your data isn't lost, and can still be accessed for other purposes (like reporting).

Therefore, ==it’s a best practice to store your data in one tool, which is separate from another tool you use to train your models==. Which tool or service is best to store your data depends on the data you have and the service you use for model training.

### Store data for model training workloads

When you use **Azure Machine Learning**, **Azure Databricks**, or **Azure Synapse Analytics** for _model training_, there are three common options for storing data, which are easily connected to all three services:

- **Azure Blob Storage**: ==Cheapest option for storing data as unstructured data==. Ideal for storing files like images, text, and JSON. Often also used to store data as CSV files, as data scientists prefer working with CSV files.
- **Azure Data Lake Storage (Gen 2)**: A more advanced version of the Azure Blob Storage. Also stores files like CSV files and images as _unstructured_ data. A data lake also implements a hierarchical namespace, which means it’s easier to give someone access to a specific file or folder. Storage capacity is virtually limitless so ideal for storing large data.
- **Azure SQL Database**: Stores data as _structured_ data. Data is read as a table and schema is defined when a table in the database is created. Ideal for data that doesn’t change over time.

By storing your data in one of these Azure storage solutions, you can easily serve the data to whichever Azure service you use for machine learning workloads. To load the data into one of these storage solutions, you can set up a pipeline to extract, transform, and load the data.

## ==Design a data ingestion solution==

A data ingestion pipeline is a sequence of tasks that move and transform the data. By creating a pipeline, you can choose to trigger the tasks manually, or schedule the pipeline when you want the tasks to be automated.

### Create a data ingestion pipeline

**Azure Synapse Analytics**

A commonly used approach to create and run pipelines for data ingestion is using the data integration feature of **Azure Synapse Analytics**, also known as **Azure Synapse Pipelines**. With Azure Synapse Pipelines you can create and schedule data ingestion pipelines through the easy-to-use UI, or by defining the pipeline in JSON format.

When you create an Azure Synapse pipeline, you can easily copy data from one source to a data store by using one of the many standard connectors.

To add a data transformation task to your pipeline, you can use a UI tool like **mapping data flow** or use a language like SQL, Python, or R.

==Azure Synapse Analytics allows you to choose between different types of compute that can handle large data transformations at scale: serverless SQL pools, dedicated SQL pools, or Spark pools.==

### Azure Databricks

Whenever you prefer a code-first tool and to use SQL, Python, or R to create your pipelines, you can also use **Azure Databricks**. ==Azure Databricks allows you to define your pipelines in a notebook, which you can schedule to run.==

Azure Databricks uses Spark clusters, which distribute the compute to transform large amounts of data in less time than when you don't use distributed compute.

### Azure Machine Learning

**Azure Machine Learning** provides compute clusters, which automatically scale up and down when needed. You can create a pipeline with the Designer, or by creating a collection of scripts. Though Azure Machine Learning pipelines are commonly used to train machine learning models, you could also use it to extract, transform, and store the data in preparation for training a machine learning model.

Whenever you want to perform all tasks within the same tool, creating and scheduling an Azure Machine Learning pipeline to run with the on-demand compute cluster may best suit your needs.

However, ==Azure Synapse Analytics and Azure Databricks offer more scalable compute that allow for transformations to be distributed across compute nodes==. Therefore, your data transformations may perform better when you execute them with either Azure Synapse Analytics or Azure Databricks instead of using Azure Machine Learning.

## Design a data ingestion solution

==A benefit of using cloud technologies is the flexibility to create and use the services that best suit your needs==. To create a solution, you can link services to each other and represent the solution in an **architecture**.

For example, a common approach for a data ingestion solution is to:

1. Extract raw data from its source (like a CRM system or IoT device).
2. Copy and transform the data with Azure Synapse Analytics.
3. Store the prepared data in an Azure Blob Storage.
4. Train the model with Azure Machine Learning.

It's a best practice to think about the architecture of a data ingestion solution before training your model. Thinking about how the data is automatically extracted and prepared for model training will help you to prepare for when your model is ready to go to production.

# Design a machine learning model training solution

- Identify machine learning tasks
- Choose a service to train a model
- Choose between compute options

As a data scientist, you want to focus your time on training machine learning models. You want to easily get access of the data you need and have sufficient compute to train a model. To be efficient and cost-effective, you need to choose the appropriate service to train a machine learning model.

To decide which service best fits your needs, you need to know what type of model you're training. By understanding the requirements of your model, you can design a model training solution, which ensures the model keeps performing as expected and stays relevant over time.

You'll learn how to design a machine learning training solution and how to choose the tools you need to train a model.

## Learning objectives

In this module, you'll learn how to:

- Identify machine learning tasks.
- Choose a service to train a model.
- Choose between compute options.

## Identify machine learning tasks

Starting with the first step, you want to **define the problem** the model will solve by understanding:

- What the model’s output should be.
- What type of machine learning task you’ll use.
- What criteria makes a model successful.

Depending on the data you have and the expected output of the model, you can identify the machine learning task. The task will determine which types of algorithms you can use to **train the model**.

1. **Classification**: Predict a categorical value.
2. **Regression**: Predict a numerical value.
3. **Time-series forecasting**: Predict future numerical values based on time-series data.
4. **Computer vision**: Classify images or detect objects in images.
5. **Natural language processing** (**NLP**): Extract insights from text.

To train a model, you have a set of algorithms that you can use, depending on the task you want to perform. To evaluate the model, you can calculate performance metrics such as accuracy or precision. The metrics available will also depend on the task your model needs to perform and will help you to decide whether a model is successful in its task.

When you know what the problem is you're trying to solve and how you'll assess the success of your model, you can choose the service to train and manage your model.

## Choose a service to train a machine learning model

There are many services available to train machine learning models. Which service you use depends on factors like:

- What type of model you need to train.
- Whether you need full control over model training.
- How much time you want to invest in model training.
- Which services are already within your organization.
- Which programming language you’re comfortable with.

Within Azure, there are several services available for training machine learning models. When you choose to work with Azure instead of training a model on a local device, you’ll have access to scalable and cost-effective compute. For example, you’ll be able to use compute only for the time needed to train a model, and not pay for the compute when it’s not used.

Some commonly used services in Azure to train machine learning models are:

https://learn.microsoft.com/en-us/training/modules/design-machine-learning-model-training-solution/3-choose-service-train

### Understand the difference between services

Choosing a service to use for training your machine learning models may be challenging. Often, multiple services would fit your scenario. There are some general guidelines to help you:

- Use Azure AI Services whenever one of the customizable prebuilt models suits your requirements, to **save time and effort**.
- Use Azure Synapse Analytics or Azure Databricks if you want to **keep all data-related** (data engineering and data science) **projects within the same service**.
- Use Azure Synapse Analytics or Azure Databricks if you need **distributed compute** for working with large datasets (datasets are large when you experience capacity constraints with standard compute). You'll need to work with [PySpark](https://spark.apache.org/docs/latest/api/python) to use the distributed compute.
- Use Azure Machine Learning or Azure Databricks when you want **full control** over model training and management.
- Use Azure Machine Learning when **Python** is your preferred programming language.
- Use Azure Machine Learning when you want an **intuitive user interface** to manage your machine learning lifecycle.

## Decide between compute options

When you want to train your own model, the most valuable resource you’ll consume is compute. Especially during model training, it’s important to choose the most suitable compute. Additionally, you should monitor compute utilization to know when to scale up or down to save on time and costs.

Though finding which virtual machine size best fits your needs is an iterative process, there are some guidelines you can follow when you start developing.

### CPU or GPU

One important decision to make when configuring compute is whether you want to use a **central processing unit** (**CPU**) or a **graphics processing unit** (**GPU**). For smaller tabular datasets, CPU will be sufficient and cheaper to use. Whenever working with unstructured data like images or text, GPUs will be more powerful and effective.

For larger amounts of tabular data, it may also be beneficial to use GPUs. When processing your data and training your model takes a long time, even with the largest CPUs compute available, you may want to consider using GPUs compute instead. There are libraries such as RAPIDs (developed by NVIDIA) which allow you to efficiently perform data preparation and model training with larger tabular datasets. As GPUs come at a higher cost than CPUs, it may require some experimentation to explore whether using GPU will be beneficial for your situation.

### General purpose or memory optimized

When you create compute resources for machine learning workloads, there are two common types of virtual machine sizes you can choose from:

- **General purpose**: Have a balanced CPU-to-memory ratio. Ideal for testing and development with smaller datasets.
- **Memory optimized**: Have a high memory-to-CPU ratio. Great for in-memory analytics, which is ideal when you have larger datasets or when you're working in notebooks.

The size of compute in Azure Machine Learning is shown as the **virtual machine size**. The sizes follow the same naming conventions as Azure Virtual Machines.

### Spark

Services like Azure Synapse Analytics and Azure Databricks offer Spark compute. Spark compute or clusters use the same sizing as virtual machines in Azure but distribute the workloads.

A Spark cluster consists of a driver node and worker nodes. Your code will initially communicate with the driver node. The work is then distributed across the worker nodes. When you use a service that distributes the work, parts of the workload can be executed in parallel, reducing the processing time. Finally, the work is summarized and the driver node communicates the result back to you.

To make optimal use of a Spark cluster, your code needs to be written in a Spark-friendly language like Scala, SQL, RSpark, or PySpark in order to distribute the workload. If you write in Python, you’ll only use the driver node and leave the worker nodes unused.

### Monitor the compute utilization

Configuring your compute resources for training a machine learning model is an iterative process. When you know how much data you have and how you want to train your model, you’ll have an idea of which compute options may best suit training your model.

Every time you train a model, you should monitor how long it takes to train the model and how much compute is used to execute your code. By monitoring the compute utilization, you’ll know whether to scale your compute up or down. If training your model takes too long, even with the largest compute size, you may want to use GPUs instead of CPUs. Alternatively, you can choose to distribute model training by using Spark compute which may require you to rewrite your training scripts.

# Design a model deployment solution

To ensure that your model is used by your target audience, you need to ==deploy your model to an endpoint.== The endpoint can be integrated into a service or application to serve the users of the model. You should design a solution for deploying the model that best meets the needs of the users and takes into account the requirements of the model being deployed.

You'll learn how to ==design a model deployment solution== and how the requirements of the deployed model can affect the way you train a model.

## Understand how model will be consumed

You should plan how you **integrate the model**, as it may affect how you train the model or what training data you use. To integrate the model, you need to deploy a model to an **endpoint**. You can ==deploy a model to an endpoint for either **real-time or batch predictions**==.

## Deploy a model to an endpoint

When you train a model, ==the goal is often to integrate the model into an application.==

To easily integrate a model into an application, you can use **endpoints**. Simply put, an endpoint can be a web address that an application can call to get a message back.

With Azure Machine Learning, you can deploy your model to an endpoint. Then you can integrate the endpoint into your own application and call the model to get the predictions in the application where you want to visualize them.

When you deploy a model to an endpoint, you have two options:

### Get real-time predictions

If you want the model to score any new data as it comes in, you need predictions in real-time.

Real-time predictions are often needed when a model is used by an application such as a mobile app or a website.

Imagine you have a website that contains your product catalog:

1. A customer selects a product on your website, such as a shirt.
2. Based on the customer's selection, the model recommends other items from the product catalog immediately. The website displays the model's recommendations.

![[Pasted image 20240819132447.png]]

A customer can select a product in the web shop at any time. You want the model to find the recommendations almost immediately. The time it takes for the web page to load and display the shirt details is the time it should take to get the recommendations or predictions. Then, when the shirt is displayed, the recommendations can also be displayed.

### Get batch predictions

If you want the model to score new data in batches, and save the results as a file or in a database, you need batch predictions.

For example, you can train a model that predicts orange juice sales for each future week. By predicting orange juice sales, you can ensure that supply is sufficient to meet expected demand.

Imagine you're visualizing all historical sales data in a report. You'll want to include the predicted sales in the same report.

![[Pasted image 20240819132521.png]]

Although orange juice is sold throughout the day, you only want to calculate the forecast once a week. You can collect the sales data throughout the week and call the model only when you have the sales data of a whole week. A collection of data points is referred to as a batch.

## Decide on real-time or batch deployment

When you deploy a model to an endpoint to integrate with an application, you can choose to design it for real-time or batch predictions.

The type of predictions you need depends on how you want to use the model's predictions

To decide whether to design a real-time or batch deployment solution, you need to consider the following questions:

- How often should predictions be generated?
- How soon are the results needed?
- Should predictions be generated individually or in batches?
- How much compute power is needed to execute the model?

## Identify the necessary frequency of scoring

A common scenario is that you're using a model to score new data. Before you can get predictions in real-time or in batch, you must first collect the new data.

There are various ways to generate or collect data. New data can also be collected at different time intervals.

For example, you can collect temperature data from an Internet of Things (IoT) device every minute. You can get transactional data every time a customer buys a product from your web shop. Or you can extract financial data from a database every three months.

Generally, there are two types of use cases:

1. You need the model to score the new data as soon as it comes in.
2. You can schedule or trigger the model to score the new data that you've collected over time.

![[Pasted image 20240819132715.png]]

Whether you want real-time or batch predictions _doesn't necessarily depend on how often new data is collected_. Instead, it depends on how often and how quickly you need the predictions to be generated.

If you need the model's predictions immediately when new data is collected, you need real-time predictions. If the model's predictions are only consumed at certain times, you need batch predictions.

## Decide on the number of predictions

Another important question to ask yourself is whether you need the predictions to be generated individually or in batches.

A simple way to illustrate the difference between individual and batch predictions is to imagine a table. Suppose you have a table of customer data where each row represents a customer. For each customer, you have some demographic data and behavioral data, such as how many products they've purchased from your web shop and when their last purchase was.

Based on this data, you can predict customer churn: whether a customer will buy from your web shop again or not.

Once you've trained the model, you can decide if you want to generate predictions:

- **Individually**: The model receives a _single row of data_ and returns whether or not that individual customer will buy again.
- **Batch**: The model receives _multiple rows of data_ in one table and returns whether or not each customer will buy again. The results are collated in a table that contains all predictions.

You can also generate individual or batch predictions when working with files. For example, when working with a computer vision model you may need to score a single image individually, or a collection of images in one batch.

## Consider the cost of compute

In addition to using compute when training a model, you also need compute when deploying a model. ==Depending on whether you deploy the model to a real-time or batch endpoint, you'll use different types of compute==. To decide whether to deploy your model to a real-time or batch endpoint, you must consider the cost of each type of compute.

==For real-time predictions==

- You need compute that is always available and able to return the results (almost) immediately. **Container** technologies like _Azure Container Instance_ (ACI) and _Azure Kubernetes Service_ (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.
- The compute is ==always on==. Once a model is deployed, you're continuously paying for the compute as you can't pause, or stop the compute as the model must always be available for immediate predictions.

==For batch predictions==

- You need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in ==parallel== batches by using multiple nodes.
- When working with compute clusters that can process data in parallel batches, the compute is provisioned by the workspace when the batch scoring is triggered, and scaled down to 0 nodes when there's no new data to process. By letting the workspace scale down an idle compute cluster, you can save significant costs.

## Decide on real-time or batch deployment

Choosing a deployment strategy for your machine learning models may be challenging, as different factors may influence your decision.

There are scenarios where you expect to need real-time predictions when batch predictions can be more cost-effective. Remember that you're continuously paying for compute with real-time deployments, even when no new predictions are generated.

If you can allow for a 5-10 minutes delay when needing immediate predictions, you can opt to deploy your model to a batch endpoint. The delay is caused in the time it needs to start the compute cluster after the endpoint is triggered. However, the compute cluster will also stop after the prediction is generated, minimizing costs and potentially being a more cost-effective solution.

Finally, you also have to consider the required compute for your model to score new data. Simpler models require less cost and time to generate predictions. More complex models may require more compute power and processing time to generate predictions. ==Therefore, you should consider how you'll deploy your model before deciding on how to train your model.==

# Design a machine learning operations solution

Learn about machine learning operations to bring a model from development to production. Identify options for monitoring and retraining when preparing a model for production.

Imagine you trained a model. The next step is to operationalize the model and to make sure whoever needs the predictions can consume them.

**Machine Learning operations** or **MLOps** help you to scale your model from a proof of concept or pilot project to production. A model in production is ready for large-scale deployment and is retrained and redeployed when necessary.

Implementing MLOps helps you to make your machine learning workloads robust and reproducible.

You'll learn about a typical MLOps architecture and what you need to consider to bring a model to production.

As a data scientist, you want to train the best machine learning model. To implement the model, you want to deploy it to an endpoint and integrate it with an application.

Over time, you may want to retrain the model. For example, you can retrain the model when you have more training data.

In general, ==once you've trained a machine learning model, you want to get the model ready for enterprise-scale==. To prepare the model and operationalize it, you want to:

- Convert the model training to a **robust** and **reproducible** pipeline.
- Test the code and the model in a **development** environment.
- Deploy the model in a **production** environment.
- **Automate** the end-to-end process.

## Set up environments for development and production

Within MLOps, similarly to DevOps, an **environment** refers to a collection of resources. These resources are used to deploy an application, or with machine learning projects, to deploy a model.

How many environments you work with, depends on your organization. Commonly, there are at least two environments: ==development==  and ==production==. Plus, you can add environments in between like a staging or _pre-production_ (_pre-prod_) environment.

![[Pasted image 20240819141419.png]]

A typical approach is to:

- Experiment with model training in the _development_ environment.
- Move the best model to the staging or pre-prod environment to deploy and test the model.
- Finally release the model to the production environment to deploy the model so that end-users can consume it.

### Organize Azure Machine Learning environments

When you implement MLOps, and work with machine learning models at a large scale, it's a best practice to work with separate environments for different stages.

Imagine your team uses a dev, pre-prod, and prod environment. ==Not everyone on your team should get access to all environments==. Data scientists may only work within the dev environment with non-production data, while machine learning engineers work on deploying the model in the pre-prod and prod environment with production data.

Having separate environments makes it easier to control access to resources. Each environment can then be associated with a separate Azure Machine Learning workspace.

![[Pasted image 20240819134703.png]]

Within Azure, you use ==role-based access control (RBAC)== to give colleagues the right level of access to the subset of resources they need to work with.

Alternatively, you can use only one Azure Machine Learning workspace. When you use one workspace for development and production, you have a smaller Azure footprint and less management overhead. However, RBAC applies to both dev and prod environments, which may mean that you're giving people too little or too much access to resources.

## Design an MLOps architecture

Bringing a model to production means you need to scale your solution and work together with other teams. Together with other data scientists, data engineers and an infrastructure team, you may decide on using the following approach:

- ==Store all data in an Azure Blob storage, managed by the data engineer.==
- ==The infrastructure team creates all necessary Azure resources, like the Azure Machine Learning workspace.==
- ==Data scientists focus on what they do best: developing and training the model (inner loop).==
- ==Machine learning engineers deploy the trained models (outer loop).==

As a result, your MLOps architecture includes the following parts:

1. **Setup**: Create all necessary Azure resources for the solution.
2. **Model development (inner loop)**: Explore and process the data to train and evaluate the model.
3. **Continuous integration**: Package and register the model.
4. **Model deployment (outer loop)**: Deploy the model.
5. **Continuous deployment**: Test the model and promote to production environment.
6. **Monitoring**: Monitor model and endpoint performance.

When you're working with larger teams, ==you're not expected to be responsible of all parts of the MLOps architecture as a data scientist==. To prepare your model for MLOps however, you should think about how to design for monitoring and retraining.

## Design for monitoring

As part of a machine learning operations (MLOps) architecture, you should think about how to monitor your machine learning solution.

**Monitoring** is beneficial in any MLOps environment. You'll want to monitor the model, the data, and the infrastructure to collect metrics that help you decide on any necessary next steps.

### Monitor the model

Monitor the performance of your model. During development, you use MLflow to train and track your machine learning models. Depending on the model you train, there are different metrics you can use to evaluate whether the model is performing as expected.

To monitor a model in production, you can use the trained model to generate predictions on a small subset of new incoming data. By generating the performance metrics on that test data, you're able to verify whether the model is still achieving its goal.

onitor for any responsible artificial intelligence (AI) issues. For example, whether the model is making fair predictions.

Before you can monitor a model, it's important to decide which performance metrics you want to monitor and what the benchmark for each metric should be. When should you be alerted that the model isn't accurate anymore?

### Monitor the data

You typically train a machine learning model using a historical dataset that is representative of the new data that your model receives when deployed. However, over time there may be trends that change the profile of the data, making your model less accurate.

This change in data profiles between current and the training data is known as ==data drift==, and it can be a significant issue for predictive models used in production. It's therefore important to be able to monitor data drift over time, and retrain models as required to maintain predictive accuracy.

### Monitor the infrastructure

Monitor the infrastructure to minimize cost and optimize performance.

Throughout the machine learning lifecycle, you use compute to train and deploy models. With machine learning projects in the cloud, ==compute may be one of your biggest expenses==. You therefore want to monitor whether you are efficiently using your compute.

For example, you can monitor the compute utilization of your compute during training and during deployment. By reviewing compute utilization, you know whether you can scale down your provisioned compute, or whether you need to scale out to avoid capacity constraints.

## Design for retraining

Generally, there are two approaches to when you want to retrain a model:

- Based on a **schedule**: when you know you always need the latest version of the model, you can decide to retrain your model every week, or every month, based on a schedule.
- Based on **metrics**: if you only want to retrain your model when necessary, you can monitor the model's performance and data drift to decide when you need to retrain the model.

In either case, you need to design for retraining. To easily retrain your model, you should prepare your code for automation.

### Prepare your code

Ideally, you should train models with **scripts** instead of notebooks. Scripts are better suited for automation. ==You can add **parameters** to a script and change input parameters like the training data or hyperparameter values==. When you parameterize your scripts, you can easily retrain the model on new data if needed.

Another important thing to prepare your code is to ==host the code in a central repository==. A repository refers to a location where all relevant files to a project are stored. With machine learning projects, Git-based repositories are ideal to achieve **source control**.

When you apply source control to your project, you can easily collaborate on a project. You can assign someone to improve the model by updating the code. You'll be able to see past changes, and you can review changes before they're committed to the main repository.

### Automate your code

When you want to automatically execute your code, you can configure Azure Machine Learning jobs to run scripts. ==In Azure Machine Learning, you can create and schedule pipelines to run scripts==.

If you want scripts to run based on a trigger or event happening outside of Azure Machine Learning, you may want to trigger the Azure Machine Learning job from another tool.

Two tools that are commonly used in MLOps projects are ==Azure DevOps and GitHub (Actions)==. Both tools allow you to create automation pipelines and can trigger Azure Machine Learning pipelines.

As a data scientist, you may prefer to work with the Azure Machine Learning Python SDK. However, when working with tools like Azure DevOps and GitHub, you may prefer to configure the necessary resources and jobs with the Azure Machine Learning CLI extension instead. The Azure CLI is designed for automating tasks and may be easier to use with Azure DevOps and GitHub.