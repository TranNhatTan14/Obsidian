How to use theÂ [pipeline()](https://huggingface.co/docs/transformers/v4.44.0/en/main_classes/pipelines#transformers.pipeline)Â for inference, load a pretrained model and preprocessor with anÂ [AutoClass](https://huggingface.co/docs/transformers/en/model_doc/auto), and quickly train a model with PyTorch or TensorFlow

Before you begin, make sure you have all the necessary libraries installed:

```python
!pip install transformers datasets evaluate accelerate

!pip install torch
```

Youâ€™ll also need to install your preferred machine learning framework:

### [Pipeline](https://huggingface.co/docs/transformers/en/quicktour#pipeline)

### [AutoClass](https://huggingface.co/docs/transformers/en/quicktour#autoclass)

Under the hood, theÂ [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.44.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification)Â andÂ [AutoTokenizer](https://huggingface.co/docs/transformers/v4.44.0/en/model_doc/auto#transformers.AutoTokenizer)Â classes work together to power theÂ [pipeline()](https://huggingface.co/docs/transformers/v4.44.0/en/main_classes/pipelines#transformers.pipeline)Â you used above. AnÂ [AutoClass](https://huggingface.co/docs/transformers/en/model_doc/auto)Â is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriateÂ `AutoClass`Â for your task and itâ€™s associated preprocessing class.

encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
print(encoding)