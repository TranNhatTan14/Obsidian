---
tags:
  - Work
---

Xác định rõ được công việc mình muốn thực hiện giúp hạn chế lan man

# Yes
## Machine Learning Operations

- Manages the deployment, monitoring, and optimization of AI/ML models in production environments.
## Machine Learning Engineer

The machine learning engineer is responsible for deploying the model. Having clear roles and responsibilities leaves no confusion as to who does what, which can greatly speed up the lifecycle.

Focuses specifically on machine learning frameworks, libraries (e.g., TensorFlow, PyTorch), and algorithms. Their expertise lies in tuning models, feature engineering, and optimization.

Designs and implements machine learning systems and algorithms.

## Data Scientist

- Analyzes and interprets complex data to provide insights and build AI models.
- Assess the performance of the machine learning model according to the performance requirements
- Develop and implement ML model to solve specific bussiness problems
- Perform hyperparameter tuning to make sure the model training process is optimized

Understanding Machine Learning Operations (MLOps) is essential for any data scientist, engineer, or leader to ==take machine learning models from a local notebook to a functioning model in production==. This course introduces you to the key processes, phases, and levels of MLOps, including ==design, development, deployment, and monitoring==. You'll discover how ==automation== enables organizations to efficiently launch, monitor, and update their machine learning models.

## Lead or PM

- Designs the overall structure of AI systems, ensuring scalability, performance, and security
- Oversees the development and deployment of AI-powered products.

## AI Research Scientist

- Conducts cutting-edge research to develop new AI algorithms, models, and technologies.

## AI Engineer

- Develops, tests, and implements AI solutions, typically involving both software and hardware aspects.

# Not

### Data Engineer

https://www.deeplearning.ai/courses/data-engineering

- Extract raw data from different sources and store in a centrailized place
- Design and maintain data pipelines to ensure efficient data flow and accessibility for analysis.
- Make sure that incoming data is validated according to data quality requirements.